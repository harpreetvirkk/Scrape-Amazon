{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"},"colab":{"name":"ScrapeSearchResult.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"GdQLsxoFEyL4","colab_type":"text"},"source":["# Script for extracting search result product details \n","\n","The script takes as input a .txt file containing a list of search keywords and then iterates through all the keywords in that list to find all their respective search results.\n","\n","For each keyword in the keyword list, it uses it to search for products on Amazon's Website and then for each product it saves its search result data and product page data in a .jsonl file."]},{"cell_type":"code","metadata":{"id":"BYhyD6xOEyL6","colab_type":"code","colab":{}},"source":["import urllib.request # FOR URL ENCODING \n","import requests # For making requests to download a webpage content\n","from selectorlib import Extractor # For extracting specific fileds from downloaded webpage\n","import json \n","import random\n","import re\n","from time import sleep\n","import os\n","import jsonlines\n","import pandas as pd\n","import datetime\n","import re"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5-xpXPNMEyL-","colab_type":"text"},"source":["#### Step 1: Read KeywordList txt file and store all keywords in a list\n","\n","**NOTE:** Before running this, change the path variable 'keywords' to point to the KeywordList .txt file. \n","\n","The following code loads a KeywordList file, and stores each into a list. "]},{"cell_type":"code","metadata":{"id":"JYg5B0XcEyL_","colab_type":"code","colab":{}},"source":["!ls ./../DATASET/KeywordLists"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MQVxUDStEyMC","colab_type":"code","colab":{}},"source":["keywords = open('./../DATASET/KeywordLists/top_100_keyword_list.txt', 'r')\n","keyword_list = []\n","\n","for k in keywords:\n","    k = k.strip(\"\\n\")\n","    keyword_list.append(k)\n","print('Keyword List: ', keyword_list[:10])\n","print('Keyword Count: ', len(keyword_list))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dYu4Sn5yEyMF","colab_type":"text"},"source":["#### Step 2: Define Headers\n","\n","Each header is a unique user agent which will be used to request the data from the website to be scraped. We use multiple user agents to ensure that if our request is rejected, we can retry.\n","\n","To create more headers, simply copy any one of the old headers and replace the 'user-agent' string with a new 'user-agent' string, which can be found online. (Eg. https://developer.chrome.com/multidevice/user-agent)"]},{"cell_type":"code","metadata":{"id":"DxpE2SWLEyMG","colab_type":"code","colab":{}},"source":["headers = [\n","           {\n","        'dnt': '1',\n","        'upgrade-insecure-requests': '1',\n","        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/42.0.2311.135 Safari/537.36 Edge/12.246',\n","        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n","        'sec-fetch-site': 'same-origin',\n","        'sec-fetch-mode': 'navigate',\n","        'sec-fetch-user': '?1',\n","        'sec-fetch-dest': 'document',\n","        'referer': 'https://www.amazon.com/',\n","        'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',\n","           },\n","           {\n","        'dnt': '1',\n","        'upgrade-insecure-requests': '1',\n","        'user-agent': 'Mozilla/5.0 (X11; CrOS x86_64 8172.45.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.64 Safari/537.36',\n","        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n","        'sec-fetch-site': 'same-origin',\n","        'sec-fetch-mode': 'navigate',\n","        'sec-fetch-user': '?1',\n","        'sec-fetch-dest': 'document',\n","        'referer': 'https://www.amazon.com/',\n","        'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',\n","           },\n","           {\n","        'dnt': '1',\n","        'upgrade-insecure-requests': '1',\n","        'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_2) AppleWebKit/601.3.9 (KHTML, like Gecko) Version/9.0.2 Safari/601.3.9',\n","        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n","        'sec-fetch-site': 'same-origin',\n","        'sec-fetch-mode': 'navigate',\n","        'sec-fetch-user': '?1',\n","        'sec-fetch-dest': 'document',\n","        'referer': 'https://www.amazon.com/',\n","        'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',\n","           },\n","           {\n","        'dnt': '1',\n","        'upgrade-insecure-requests': '1',\n","        'user-agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/47.0.2526.111 Safari/537.36',\n","        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n","        'sec-fetch-site': 'same-origin',\n","        'sec-fetch-mode': 'navigate',\n","        'sec-fetch-user': '?1',\n","        'sec-fetch-dest': 'document',\n","        'referer': 'https://www.amazon.com/',\n","        'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',\n","           },\n","           {\n","        'dnt': '1',\n","        'upgrade-insecure-requests': '1',\n","        'user-agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:15.0) Gecko/20100101 Firefox/15.0.1',\n","        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n","        'sec-fetch-site': 'same-origin',\n","        'sec-fetch-mode': 'navigate',\n","        'sec-fetch-user': '?1',\n","        'sec-fetch-dest': 'document',\n","        'referer': 'https://www.amazon.com/',\n","        'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',\n","           },\n","           {\n","        'dnt': '1',\n","        'upgrade-insecure-requests': '1',\n","        'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.1 Safari/605.1.15',\n","        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n","        'sec-fetch-site': 'same-origin',\n","        'sec-fetch-mode': 'navigate',\n","        'sec-fetch-user': '?1',\n","        'sec-fetch-dest': 'document',\n","        'referer': 'https://www.amazon.com/',\n","        'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',\n","           },\n","           {\n","        'dnt': '1',\n","        'upgrade-insecure-requests': '1',\n","        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36 Edge/16.16299',\n","        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n","        'sec-fetch-site': 'same-origin',\n","        'sec-fetch-mode': 'navigate',\n","        'sec-fetch-user': '?1',\n","        'sec-fetch-dest': 'document',\n","        'referer': 'https://www.amazon.com/',\n","        'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',\n","           },\n","           {\n","        'dnt': '1',\n","        'upgrade-insecure-requests': '1',\n","        'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.70 Safari/537.36',\n","        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n","        'sec-fetch-site': 'same-origin',\n","        'sec-fetch-mode': 'navigate',\n","        'sec-fetch-user': '?1',\n","        'sec-fetch-dest': 'document',\n","        'referer': 'https://www.amazon.com/',\n","        'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',\n","           },\n","           {\n","        'dnt': '1',\n","        'upgrade-insecure-requests': '1',\n","        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.70 Safari/537.36',\n","        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n","        'sec-fetch-site': 'same-origin',\n","        'sec-fetch-mode': 'navigate',\n","        'sec-fetch-user': '?1',\n","        'sec-fetch-dest': 'document',\n","        'referer': 'https://www.amazon.com/',\n","        'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',\n","           },\n","           {\n","        'dnt': '1',\n","        'upgrade-insecure-requests': '1',\n","        'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:70.0) Gecko/20100101 Firefox/70.0',\n","        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n","        'sec-fetch-site': 'same-origin',\n","        'sec-fetch-mode': 'navigate',\n","        'sec-fetch-user': '?1',\n","        'sec-fetch-dest': 'document',\n","        'referer': 'https://www.amazon.com/',\n","        'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',\n","           },\n","           {\n","        'dnt': '1',\n","        'upgrade-insecure-requests': '1',\n","        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:70.0) Gecko/20100101 Firefox/70.0',\n","        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n","        'sec-fetch-site': 'same-origin',\n","        'sec-fetch-mode': 'navigate',\n","        'sec-fetch-user': '?1',\n","        'sec-fetch-dest': 'document',\n","        'referer': 'https://www.amazon.com/',\n","        'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',\n","           },\n","           {\n","        'dnt': '1',\n","        'upgrade-insecure-requests': '1',\n","        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Safari/537.36',\n","        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n","        'sec-fetch-site': 'same-origin',\n","        'sec-fetch-mode': 'navigate',\n","        'sec-fetch-user': '?1',\n","        'sec-fetch-dest': 'document',\n","        'referer': 'https://www.amazon.com/',\n","        'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',\n","           },\n","           {\n","        'dnt': '1',\n","        'upgrade-insecure-requests': '1',\n","        'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36',\n","        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n","        'sec-fetch-site': 'same-origin',\n","        'sec-fetch-mode': 'navigate',\n","        'sec-fetch-user': '?1',\n","        'sec-fetch-dest': 'document',\n","        'referer': 'https://www.amazon.com/',\n","        'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',\n","           },\n","           {\n","        'dnt': '1',\n","        'upgrade-insecure-requests': '1',\n","        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36 OPR/68.0.3618.165',\n","        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n","        'sec-fetch-site': 'same-origin',\n","        'sec-fetch-mode': 'navigate',\n","        'sec-fetch-user': '?1',\n","        'sec-fetch-dest': 'document',\n","        'referer': 'https://www.amazon.com/',\n","        'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',\n","           },\n","           {\n","        'dnt': '1',\n","        'upgrade-insecure-requests': '1',\n","        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Trident/7.0; rv:11.0) like Gecko',\n","        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n","        'sec-fetch-site': 'same-origin',\n","        'sec-fetch-mode': 'navigate',\n","        'sec-fetch-user': '?1',\n","        'sec-fetch-dest': 'document',\n","        'referer': 'https://www.amazon.com/',\n","        'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',\n","           },\n","           {\n","        'dnt': '1',\n","        'upgrade-insecure-requests': '1',\n","        'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36 Edg/83.0.478.37',\n","        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n","        'sec-fetch-site': 'same-origin',\n","        'sec-fetch-mode': 'navigate',\n","        'sec-fetch-user': '?1',\n","        'sec-fetch-dest': 'document',\n","        'referer': 'https://www.amazon.com/',\n","        'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',\n","           }\n","]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5isZY_7HEyMJ","colab_type":"text"},"source":["#### Step 3: Read Extractor Files\n","\n","The extractor (.yml) files contain *css id* information about the fields which we intend to extract from the scarped website. Here, the two extractor files are:\n","##### 1. keyword_search_product_list.yml\n","From the scraped webpage, this extractor file extracts the main *css division* which contains all the individual (child) products. Once the main div is scraped, it extracts all the child divisions (products) contained in it.\n","##### 2. keyword_search_product_page.yml \n","From the scraped product page, this extractor file extracts the all the fields that are relevant to the product on the given page.\n","##### 3. nextpg.yml\n","Extracts the 'next' button from the website, to check if its disabled. If it is disabled, it means that we have reached the end of the product list for the current brand. We then move onto the next brand to continue our scraping."]},{"cell_type":"code","metadata":{"id":"j-6MC4hGEyMK","colab_type":"code","colab":{}},"source":["e = Extractor.from_yaml_file('./Extractor/keyword_search_product_list.yml')\n","l = Extractor.from_yaml_file('./Extractor/nextpg.yml')\n","p = Extractor.from_yaml_file('./Extractor/keyword_search_product_page.yml')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xeLyGxcPEyMN","colab_type":"text"},"source":["#### Step 4: Define scrape function for search results scraping\n","\n","**NOTE:** Set the variables MAX_TRIALS & ERROR_THRESHHOLD according to your preferences. \n","\n","A high MAX_TRIALS will slow down the scraping as it will scrape those pages without actually any data multiple times too, but it will reduce the chances of error. \n","A low ERROR_THRESHHOLD will also slow down the scraping, as VPN will need to changed multiple times. However, it will reduce the chances missing data due to errors. \n","\n","The function scrape_SearchResult(url) downloads the webpage at the given url (here: search result page) using requests module, and looks for products on the page. If it finds any product, it extracts the required fields and returns the data. If no product is found, it continues to randomly select a new header and retry scraping untill the limit MAX_TRIALS is reached, where it concludes that the page does not contain any data.\n","\n","These multiple trials are required, as amazon often blocks a user for repeqatedly making requests using the same user agent. "]},{"cell_type":"code","metadata":{"id":"HUE_qoGSEyMN","colab_type":"code","colab":{}},"source":["MAX_TRIALS_A = 25  # Set the max number of trials to perform here.\n","ERROR_COUNT_A = 1 # Used for keeping a count of errors, if the count exceeds threshhold, the user is asked to\n","                # change the vpn\n","ERROR_THRESHHOLD_A = 5 # Number of pages with missed information allowed after which vpn change is required\n","def scrape_SearchResult(url):  \n","    global ERROR_COUNT_A\n","    \n","    '''\n","    This function downloads the webpage at the given url using requests module.\n","    \n","    Parameters:\n","    url (string): URL of webpage to scrape\n","    Returns: \n","    string: If the URL contains products, returns the html of the webpage as text, else returns 'False'.\n","    '''\n","    \n","    # Download the page using requests\n","    print(\"Downloading %s\"%url)\n","    trial = 0\n","    while(True):\n","        # Ask to change vpn every ERROR_THRESHHOLD pages without results to ensure data is not missed because of being blocked\n","        if ERROR_COUNT_A % ERROR_THRESHHOLD_A == 0:\n","            _ = input('Please Change VPN and enter \\'DONE\\' to continue')\n","            ERROR_COUNT_A += 1\n","        if trial == MAX_TRIALS_A:  \n","            print(\"Max trials exceeded yet no Data found on this page!\")\n","            ERROR_COUNT_A += 1\n","            return 'False'\n","        trial = trial + 1\n","        print(\"Trial no:\", trial)\n","        \n","        # Get the html data from the url\n","        while True:\n","            try:\n","                r = requests.get(url, headers=random.choice(headers), timeout = 15) \n","                \n","                # We use product_list.yml extractor to extract the product details from the html data text\n","                data = e.extract(r.text) \n","                # If the products div in the scraped html is not empty, return html text. \n","                #If the products div in the scraped html is empty, retry with new user agent.\n","                if (data['products'] != None): \n","                    return r.text\n","                else:\n","                    print(\"Retrying with new user agent!\")\n","                    break\n","            except requests.exceptions.RequestException as err:\n","                print('Error Detected: ', err)\n","                print('Retrying after 30 seconds')\n","                sleep(30)\n","                continue\n","            except requests.exceptions.HTTPError as err:\n","                print('Error Detected: ', err)\n","                print('Retrying after 30 seconds')\n","                sleep(30)\n","                continue\n","            except requests.exceptions.ConnectionError as err:\n","                print('Error Detected: ', err)\n","                print('Retrying after 30 seconds')\n","                sleep(30)\n","                continue\n","            except requests.exceptions.Timeout as err:\n","                print('Error Detected: ', err)\n","                print('Retrying after 30 seconds')\n","                sleep(30)\n","                continue"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NDmltsVIEyMQ","colab_type":"text"},"source":["#### Step 5: Define scrape function for product page scraping\n","\n","**NOTE:** Set the variables MAX_TRIALS & ERROR_THRESHHOLD according to your preferences. \n","\n","A high MAX_TRIALS will slow down the scraping as it will scrape those pages without actually any data multiple times too, but it will reduce the chances of error. \n","A low ERROR_THRESHHOLD will also slow down the scraping, as VPN will need to changed multiple times. However, it will reduce the chances missing data due to errors. \n","\n","The function scrape_ProductPage(url) downloads the webpage at the given url (here: product page) using requests module, and looks for the specific fileds defigned in the extractor file product_page.yml. If a Title for the product on the page is not found, it continues to randomly select a new header and retry scraping untill the limit MAX_TRIALS is reached, where it reports that the page does not contain any data.\n","\n","These multiple trials are required, as amazon often blocks a user for repeatedly making requests using the same user agent. "]},{"cell_type":"code","metadata":{"id":"kSgEiZ6qEyMR","colab_type":"code","colab":{}},"source":["MAX_TRIALS_B = 20  # Set the max number of trials to perform here.\n","ERROR_COUNT_B = 1 # Used for keeping a count of errors, if the count exceeds threshhold, the user is asked to\n","                # change the vpn\n","ERROR_THRESHHOLD_B = 25 # Number of pages with missed information allowed after which vpn change is required\n","\n","def scrape_ProductPage(url):\n","    global ERROR_COUNT_B\n","    '''\n","    This function downloads the webpage at the given url using requests module.\n","    \n","    Parameters:\n","    url (string): URL of webpage to scrape\n","    Returns: \n","    string: If the URL contains products, returns the html of the webpage as text, else returns 'False'.\n","    '''\n","    \n","    # Download the page using requests\n","    print(\"Downloading %s\"%url)\n","    trial = 0\n","    while(True):\n","        \n","        # Ask to change vpn every (ERROR_THRESHHOLD_B) pages without results to ensure data is not missed because of being blocked\n","        if ERROR_COUNT_B % ERROR_THRESHHOLD_B == 0:\n","            _ = input('Please Change VPN and press enter')\n","            ERROR_COUNT_B += 1\n","        if trial == MAX_TRIALS_B:  \n","            print(\"Max trials exceeded yet no Data found on this page!\")\n","            ERROR_COUNT_B += 1\n","            return 'False'\n","        trial = trial + 1\n","        print(\"Trial no:\", trial)\n","        \n","        # Get the html data from the url\n","        while True:\n","            try:\n","                r = requests.get(url, headers=random.choice(headers), timeout = 15) \n","                \n","                # We use product_list.yml extractor to extract the product details from the html data text\n","                data = p.extract(r.text) \n","                # If the products title in the scraped html is not empty, return extracted details as dict. \n","                # If the products title in the scraped html is empty, retry with new user agent.\n","                if data['Title'] != None:\n","                    return (p.extract(r.text))\n","                else:\n","                    print(\"Retrying with new user agent!\")\n","                    break\n","            except requests.exceptions.RequestException as err:\n","                print('Error Detected: ', err)\n","                print('Retrying after 30 seconds')\n","                sleep(30)\n","                continue\n","            except requests.exceptions.HTTPError as err:\n","                print('Error Detected: ', err)\n","                print('Retrying after 30 seconds')\n","                sleep(30)\n","                continue\n","            except requests.exceptions.ConnectionError as err:\n","                print('Error Detected: ', err)\n","                print('Retrying after 30 seconds')\n","                sleep(30)\n","                continue\n","            except requests.exceptions.Timeout as err:\n","                print('Error Detected: ', err)\n","                print('Retrying after 30 seconds')\n","                sleep(30)\n","                continue"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2c36xZh-EyMT","colab_type":"text"},"source":["#### Step 6: Initialise path of output file\n","\n","**NOTE:** Set the File Name accoring to what is being scraped here\n","\n","Eg: SearchResult_ApparioGeneric or SearchResult_Top100India"]},{"cell_type":"code","metadata":{"id":"2cAOW9REEyMU","colab_type":"code","colab":{}},"source":["FileName = input('Enter a Filename for output file!\\n')\n","\n","outfile_path = str('./ScriptOutput/DATASET/' + str(FileName) + '.jsonl')  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pUmwUJ__EyMX","colab_type":"text"},"source":["#### Step 7: Define cleaning functions"]},{"cell_type":"code","metadata":{"id":"jgc4OGN4EyMX","colab_type":"code","colab":{}},"source":["def CleanRating(s):\n","    '''\n","    Here, the input is rating in a string format, eg: \"3.3 out of 5 stars\".\n","    The function converts it to a float, eg: '3.3'\n","    '''\n","    if s is not None:\n","        try:\n","            return float(s.split(' ')[0])\n","        except ValueError:\n","            return None\n","        except AttributeError:\n","            return None\n","    else:\n","        return None\n","\n","def CleanRatingCount(s):\n","    '''\n","    Here, the input is RatingCount in a string format, eg: \"336 ratings\".\n","    The function converts it to a float, eg: '336'\n","    '''\n","    if s is not None:\n","        return float(s.split(' ')[0].replace(',', ''))\n","    else:\n","        return float(0)\n","\n","def CleanAnsweredQuestionsCount(s):\n","    '''\n","    Here, the input is AnsweredQuestionsCount in a string format, eg: \"336 answered questions\".\n","    The function converts it to a float, eg: '336'\n","    '''\n","    if s is not None:\n","        try:\n","            return float(s.split(' ')[0].replace(',', '').replace('+', ''))\n","        except ValueError:\n","            return float(0)\n","        except AttributeError:\n","            return float(0)\n","    else:\n","        return float(0)\n","    \n","def CleanAmazonPrice(s):\n","    '''\n","    Here, the input is AmazonPrice in a string format, eg: \"₹ 336.00\".\n","    The function converts it to a float, eg: '336'\n","    '''\n","    if s is not None:\n","        print(s)\n","        s = s.replace('₹', '').replace(',', '').replace(r'\\x', '').replace('a', '')\n","        return float(s.strip().split(' ')[0])\n","    else:\n","        return s\n","    \n","def CleanMRP(s):\n","    '''\n","    Here, the input is MRP in a string format, eg: \"₹ 336.00\".\n","    The function converts it to a float, eg: '336'\n","    '''\n","    if s is not None:\n","        print(s)\n","        s = s.replace('₹', '').replace(',', '').replace(r'\\x', '').replace('a', '')\n","        return float(s.strip().split(' ')[0])\n","    else:\n","        return s\n","def CleanDiscount(s):\n","    '''\n","    Here, the input is Savings in a string format, eg: \"₹ 336.00 (50% Off)\".\n","    The function converts it to a float, eg: '50'\n","    '''\n","    if s is not None:\n","        if re.search(re.compile(r'\\(.*\\)'), s):\n","            return int((re.search(re.compile(r'\\(.*\\)'), s).group(0)).replace('(', '').replace(')', '').replace('%', '').replace(',', ''))\n","        else:\n","            return s\n","    else:\n","        return s\n","\n","def CleanSavings(s):\n","    '''\n","    Here, the input is Savings in a string format, eg: \"₹ 336.00 (50% Off)\".\n","    The function converts it to a float, eg: '336'\n","    '''\n","    if s is not None:\n","        s = s.replace('₹', '').replace(',', '').replace(r'\\x', '').replace('a', '')\n","        return float(s.split(' ')[0])\n","    else:\n","        return s\n","    \n","def CleanKeywords(s):\n","    '''\n","    Here, the input is Breadcrumbs in a string format, \n","    eg: 'Electronics  > Home Audio  > Speakers  > 10.or Crafted for Amazon Rave Portable Wireless Bluetooth Speaker'\n","    The function converts it to a list, seperating it based on the '>' symbol.\n","    '''\n","    if type(s) == float:\n","        if math.isnan(s):\n","            return None\n","    else:\n","        if s is not None:\n","            if '›' in s:\n","                k = list(s.split('›'))\n","            else:\n","                k = list(s.split('> '))\n","            return k"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"85K_nH2uEyMa","colab_type":"text"},"source":["#### Step 8: Define Keyword Type:\n","\n","Eg: Top 100 Germany \n","\n","Eg: Generic Appario"]},{"cell_type":"code","metadata":{"id":"rc1YPkKOEyMa","colab_type":"code","colab":{}},"source":["# Keyword Type\n","# Example: APPARIO GENERIC\n","# Example: TOP 100 UK\n","# Example: TOP 100 INDIA\n","KeywordType = input(\"Enter Keyword Type\\n\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"87uU-B3HEyMd","colab_type":"text"},"source":["#### Step 8: Begin main scraping:"]},{"cell_type":"code","metadata":{"id":"gRkt4BXYEyMd","colab_type":"code","colab":{}},"source":["MIN_NUM_OF_PRODUCTS_TO_SCRAPE = 80\n","\n","with open(outfile_path,'a') as outfile:\n","    for k in keyword_list:\n","        pg_number = 1\n","        search_rank = 1\n","        if k == 'EOF':\n","            break\n","        \n","        while True:\n","            \n","            if search_rank >= MIN_NUM_OF_PRODUCTS_TO_SCRAPE + 1:\n","                break\n","                \n","            # To account for differnt urls based on page number\n","            if pg_number == 1:\n","                url = str(\"https://www.amazon.in/s?k=\"+str(k))\n","            else:\n","                url = str(\"https://www.amazon.in/s?k=\"+str(k)+\"&page=\"+ str(pg_number))\n","            \n","            data_text = scrape_SearchResult(url)\n","            if data_text == 'False':\n","                break\n","            \n","            else:\n","                # Extract all product details in a dict 'data' using the extractor file\n","                data = e.extract(data_text)\n","                \n","                # Save html text to file\n","                html_files_path = str('./ScriptOutput/HTML/'+ str(FileName) + '/' + str(k) +'/Page_'+str(pg_number)+'.html')\n","                os.makedirs(os.path.dirname(html_files_path), exist_ok=True)\n","                with open(html_files_path, 'w') as file:\n","                    file.write(data_text)\n","                \n","                for product in data['products']:\n","                    product['SearchResultPosition'] = search_rank\n","                    product['KeywordType'] = KeywordType\n","                    search_rank += 1\n","                    product['SearchKeyword'] = k\n","                    product['SearchUrl'] = url\n","                    date = datetime.datetime.now()\n","                    product['Timestamp'] = date.strftime(\"%c\")\n","                    if product['Label'] == 'Amazon\\'s':\n","                        product['Label'] = 'Amazon\\'s Choice'\n","                    if 'www.amazon.in' in product['ProductPageUrl']:\n","                        data = scrape_ProductPage(product['ProductPageUrl'])\n","                    else:\n","                        data = scrape_ProductPage('https://www.amazon.in'+ product['ProductPageUrl'])\n","                        product['ProductPageUrl'] = 'https://www.amazon.in'+ product['ProductPageUrl']\n","                    if data == 'False':\n","                        product['Brand'] = None\n","                        product['MRP'] = None\n","                        product['AmazonPrice'] = None\n","                        product['DiscountPercentage'] = None\n","                        product['Rating'] = None\n","                        product['RatingCount'] = None\n","                        product['Savings'] = None\n","                        product['ShortDescription'] = None\n","                        product['ProductDescription'] = None\n","                        product['BestSellerRank'] = None\n","                        product['DateFirstAvailable'] = None\n","                        product['Breadcrumbs'] = None\n","                        product['Availability'] = None\n","                        product['Seller'] = None\n","                        product['Keywords'] = None\n","                        product['FullfilledBy'] = None\n","                        if re.search('B0.{8}', product['ProductPageUrl']):\n","                            product['ASIN'] = re.search('B0.{8}', product['ProductPageUrl']).group(0)\n","                        else:\n","                            product['ASIN'] = None\n","                        print(\"Saving Product: %s\"%product['Title'])\n","                        print(product)\n","                        json.dump(product,outfile)\n","                        outfile.write(\"\\n\")\n","                        continue\n","                    product['Brand'] = data['Brand']\n","                    product['MRP'] = CleanMRP(data['MRP'])\n","                    product['Rating'] = CleanRating(data['Rating'])\n","                    product['RatingCount'] = CleanRatingCount(data['RatingCount'])\n","                    product['AnsweredQuestionsCount'] = CleanAnsweredQuestionsCount(data['AnsweredQuestionsCount'])\n","                    product['AmazonPrice'] = CleanAmazonPrice(data['AmazonPrice'])\n","                    product['DiscountPercentage'] = CleanDiscountPercentage(data['Savings'])\n","                    product['Savings'] = CleanSavings(data['Savings'])\n","                    product['ShortDescription'] = data['ShortDescription']\n","                    product['ProductDescription'] = data['ProductDescription']\n","                    product['BestSellerRank'] = data['BestSellerRank']\n","                    product['DateFirstAvailable'] = data['DateFirstAvailable']\n","                    product['Breadcrumbs'] = data['Breadcrumbs']\n","                    product['Keywords'] = CleanKeywords(data['Breadcrumbs'])\n","                    product['Seller'] = data['Seller']\n","                    product['FullfilledBy'] = data['FullfilledBy']\n","                    if product['AmazonPrice'] is not None:\n","                        product['Availability'] = 'Available'\n","                    else:\n","                        product['Availability'] = 'Currently Unavailable'\n","                    if re.search('B0.{8}', product['ProductPageUrl']):\n","                        product['ASIN'] = re.search('B0.{8}', product['ProductPageUrl']).group(0)\n","                    else:\n","                        if re.search('/dp/\\d*/', product['ProductPageUrl']):\n","                            product['ASIN'] = re.search('/dp/\\d*/', product['ProductPageUrl']).group(0).replace('/dp/', '').replace('/', '')\n","                        else:\n","                            product['ASIN'] = None\n","                    print(\"Saving Product: %s\"%product['Title'])\n","                    print(product)\n","                    json.dump(product,outfile)\n","                    outfile.write(\"\\n\")\n","\n","                # If next page is not available, break and go to next brand                  \n","                if l.extract(data_text)['last'] == 'Next →':\n","                    break\n","                elif data_text == 'False':\n","                    break\n","                else:\n","                    pg_number += 1 # Incrementing page numbe\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hiLCr3zzEyMh","colab_type":"text"},"source":["## Step 8: Read Jsonl File"]},{"cell_type":"code","metadata":{"id":"R9Nv3c-sEyMh","colab_type":"code","colab":{}},"source":["Search_Result_file = open(outfile_path, 'r')\n","\n","Search_Result_List = []\n","reader = jsonlines.Reader(Search_Result_file)\n","for item in reader.iter():\n","    Search_Result_List.append(item)\n","    \n","df = pd.DataFrame(Search_Result_List)\n","print(df.count())\n","df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fGrBFmI7EyMk","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}